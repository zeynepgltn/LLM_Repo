{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4375b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5db2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow pypı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ffb328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\zeyne\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\zeyne\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'c:\\Users\\zeyne\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'c:\\Users\\zeyne\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch tiktoken datasets matplotlib -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a9a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ef5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"The capital of France is\"\n",
    "text2=\"The dog chased the cat\"\n",
    "text3=\"The cat chased the dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03546138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'capital', 'of', 'France', 'is']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "#en ilkel tokenizer\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ba4b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 6, 9, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenizer.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "\n",
    "def tokenize2(text):\n",
    "    parts = text.split()\n",
    "    ids= []\n",
    "    for part in parts:\n",
    "        if part in vocab:\n",
    "            ids.append(vocab[part])\n",
    "        else:\n",
    "            ids.append(8)  # Unknown token\n",
    "    return ids\n",
    "\n",
    "token_ids1=tokenize2(text2)\n",
    "token_ids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa5ccb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog chased the cat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab = {v: part for part, v in vocab.items()}\n",
    "\n",
    "def detokenize(token_ids1):\n",
    "    words = \"\"\n",
    "    for id in token_ids1:  \n",
    "        part = reverse_vocab[id]\n",
    "        words += part + \" \"\n",
    "    return words.strip()\n",
    "\n",
    "# Example usage\n",
    "detokenize(token_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5df76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26243443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "gpt_ids=enc.encode(text)\n",
    "gpt_ids\n",
    "\n",
    "gpt_words=enc.decode(gpt_ids)\n",
    "gpt_words\n",
    "\n",
    "enc.decode([67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abe773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4bc0426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc=tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80b58218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")  # buraya kendi token'ını yapıştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9100f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zeyne\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-27b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d69257",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_ids=processor.tokenizer.encode(text)\n",
    "gemma_ids\n",
    "\n",
    "gemma_words=processor.tokenizer.decode(gemma_ids)[5:]\n",
    "gemma_words\n",
    "\n",
    "processor.tokenizer.to_json_file(\"gemma_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db21677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d320918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "processor.tokenizer.save_vocabulary(\"gemma_tokenizer\")\n",
    "processor.tokenizer.get_vocab()\n",
    "\n",
    "with open(\"gemma_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processor.tokenizer.get_vocab(), f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"tokenizer.json\")\n",
    "encoded = tokenizer.encode(\"states\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\",\"r\") as f:\n",
    "    text=f.read()\n",
    "\n",
    "tokens=tokenizer.encode(text)\n",
    "tokens\n",
    "\n",
    "tokens=tokenizer.decode(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3357ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --only-binary=all sentencepiece -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23169c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([39, 0, 47, 42, 16, 17, 39, 0, 48, 41, 44, 49, 42, 10],\n",
       " ['▁',\n",
       "  'T',\n",
       "  'h',\n",
       "  'e',\n",
       "  '▁capital',\n",
       "  '▁of',\n",
       "  '▁',\n",
       "  'F',\n",
       "  'r',\n",
       "  'a',\n",
       "  'n',\n",
       "  'c',\n",
       "  'e',\n",
       "  '▁is'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Doğru sınıf adı: SentencePieceTrainer (SentenPieceTrainer değil)\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"text.txt\",\n",
    "    model_prefix=\"spm_tokenizer\", \n",
    "    vocab_size=64,\n",
    "    model_type=\"bpe\"\n",
    ")\n",
    "\n",
    "\n",
    "# Tokenizer modelini yükle\n",
    "spm_tokenizer = spm.SentencePieceProcessor(model_file=\"spm_tokenizer.model\")\n",
    "\n",
    "# Encode işlemi (ID ve token)\n",
    "spm_ids = spm_tokenizer.Encode(text)\n",
    "spm_tokens = spm_tokenizer.Encode(text, out_type=str)\n",
    "\n",
    "# Sonuçları göster\n",
    "spm_ids, spm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e1644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e18888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5440c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Zeynepgltn/hf_tokenizer/commit/7bd09ff7ef4630dd8e3818b92e62b12a832f4f69', commit_message='Upload tokenizer', commit_description='', oid='7bd09ff7ef4630dd8e3818b92e62b12a832f4f69', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Zeynepgltn/hf_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='Zeynepgltn/hf_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. GEREKLİ KÜTÜPHANELER\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import login, create_repo\n",
    "\n",
    "# 2. HUGGING FACE GİRİŞ\n",
    "login(\"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\")  # Token'ı güvenli tut!\n",
    "\n",
    "# 3. TOKENIZER OLUŞTUR\n",
    "hf_tokenizer = Tokenizer(BPE())\n",
    "hf_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=64,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# 4. TEXT DOSYASINDAN EĞİT\n",
    "hf_tokenizer.train([\"text.txt\"], trainer)\n",
    "\n",
    "# 5. JSON'A KAYDET\n",
    "hf_tokenizer.save(\"hf_tokenizer.json\")\n",
    "\n",
    "# 6. TRANSFORMERS FAST FORMATINA ÇEVİR\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"hf_tokenizer.json\")\n",
    "\n",
    "fast_tokenizer.add_special_tokens({\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"mask_token\": \"[MASK]\"\n",
    "})\n",
    "\n",
    "# 7. HUGGING FACE REPO'YU OLUŞTUR & GÖNDER\n",
    "repo_id = \"Zeynepgltn/hf_tokenizer\"\n",
    "create_repo(repo_id, exist_ok=True)  # Repo yoksa oluşturur\n",
    "\n",
    "fast_tokenizer.push_to_hub(repo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf0139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
